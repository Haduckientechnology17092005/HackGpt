#!/usr/bin/env python3
"""
Zero-Day Detection System for HackGPT
Machine learning-based anomaly detection for potential zero-day vulnerabilities
"""

import os
import json
import logging
import hashlib
import pickle
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime, timedelta
import re
from collections import Counter

# ML imports
try:
    from sklearn.ensemble import IsolationForest, RandomForestClassifier
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.cluster import DBSCAN
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics.pairwise import cosine_similarity
    import pandas as pd
except ImportError as e:
    logging.warning(f"ML dependencies not available for zero-day detection: {e}")

@dataclass
class ZeroDayCandidate:
    id: str
    confidence_score: float
    anomaly_type: str
    description: str
    indicators: List[str]
    behavioral_patterns: List[str]
    technical_details: Dict[str, Any]
    recommended_actions: List[str]
    severity_prediction: str

@dataclass
class BehavioralPattern:
    pattern_id: str
    pattern_type: str
    frequency: int
    confidence: float
    description: str
    first_seen: datetime
    last_seen: datetime

class BehavioralAnalyzer:
    """Analyzes behavioral patterns in scan results"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.known_patterns = {}
        self.pattern_cache = {}
        
    def analyze_behavioral_patterns(self, scan_results: Dict[str, Any]) -> List[BehavioralPattern]:
        """Analyze behavioral patterns in scan results"""
        patterns = []
        
        # Analyze network behavior patterns
        network_patterns = self._analyze_network_behavior(scan_results)
        patterns.extend(network_patterns)
        
        # Analyze service response patterns
        service_patterns = self._analyze_service_responses(scan_results)
        patterns.extend(service_patterns)
        
        # Analyze timing patterns
        timing_patterns = self._analyze_timing_patterns(scan_results)
        patterns.extend(timing_patterns)
        
        # Analyze error patterns
        error_patterns = self._analyze_error_patterns(scan_results)
        patterns.extend(error_patterns)
        
        return patterns
    
    def _analyze_network_behavior(self, scan_results: Dict[str, Any]) -> List[BehavioralPattern]:
        """Analyze network-level behavioral patterns"""
        patterns = []
        
        # Check for unusual port combinations
        if 'nmap' in scan_results:
            nmap_data = scan_results['nmap'].get('stdout', '')
            open_ports = re.findall(r'(\d+)/tcp\s+open', nmap_data)
            
            if open_ports:
                port_combination = tuple(sorted(map(int, open_ports)))
                
                # Check if this combination is unusual
                if self._is_unusual_port_combination(port_combination):
                    patterns.append(BehavioralPattern(
                        pattern_id=f"unusual_ports_{hashlib.md5(str(port_combination).encode()).hexdigest()[:8]}",
                        pattern_type="network_anomaly",
                        frequency=1,
                        confidence=0.7,
                        description=f"Unusual port combination detected: {open_ports}",
                        first_seen=datetime.utcnow(),
                        last_seen=datetime.utcnow()
                    ))
        
        # Check for service version anomalies
        if 'whatweb' in scan_results:
            whatweb_data = scan_results['whatweb'].get('stdout', '')
            versions = re.findall(r'(\w+)[\s/](\d+\.\d+[\.\d]*)', whatweb_data)
            
            for service, version in versions:
                if self._is_unusual_version(service, version):
                    patterns.append(BehavioralPattern(
                        pattern_id=f"unusual_version_{service}_{version}",
                        pattern_type="version_anomaly",
                        frequency=1,
                        confidence=0.6,
                        description=f"Unusual {service} version detected: {version}",
                        first_seen=datetime.utcnow(),
                        last_seen=datetime.utcnow()
                    ))
        
        return patterns
    
    def _analyze_service_responses(self, scan_results: Dict[str, Any]) -> List[BehavioralPattern]:
        """Analyze service response patterns"""
        patterns = []
        
        # Analyze HTTP response patterns
        if 'nikto' in scan_results:
            nikto_data = scan_results['nikto'].get('stdout', '')
            
            # Look for unusual response codes
            response_codes = re.findall(r'HTTP/\d\.\d\s+(\d+)', nikto_data)
            code_frequency = Counter(response_codes)
            
            for code, freq in code_frequency.items():
                if self._is_unusual_response_code(code, freq):
                    patterns.append(BehavioralPattern(
                        pattern_id=f"unusual_http_response_{code}",
                        pattern_type="http_anomaly",
                        frequency=freq,
                        confidence=0.65,
                        description=f"Unusual HTTP response code pattern: {code} ({freq} occurrences)",
                        first_seen=datetime.utcnow(),
                        last_seen=datetime.utcnow()
                    ))
        
        return patterns
    
    def _analyze_timing_patterns(self, scan_results: Dict[str, Any]) -> List[BehavioralPattern]:
        """Analyze timing patterns in responses"""
        patterns = []
        
        # This would analyze response times, but requires more detailed timing data
        # For now, return empty list
        return patterns
    
    def _analyze_error_patterns(self, scan_results: Dict[str, Any]) -> List[BehavioralPattern]:
        """Analyze error message patterns"""
        patterns = []
        
        # Analyze error messages across all tools
        all_output = ""
        for tool, result in scan_results.items():
            if isinstance(result, dict) and 'stdout' in result:
                all_output += result['stdout'] + "\n"
            if isinstance(result, dict) and 'stderr' in result:
                all_output += result['stderr'] + "\n"
        
        # Look for unusual error patterns
        error_patterns = [
            (r'stack\s+trace', 'stack_trace_leak'),
            (r'debug\s+information', 'debug_info_leak'),
            (r'internal\s+server\s+error.*detail', 'detailed_error_leak'),
            (r'database\s+error.*query', 'database_error_leak'),
            (r'permission\s+denied.*path', 'path_disclosure'),
        ]
        
        for pattern, pattern_type in error_patterns:
            matches = re.findall(pattern, all_output, re.IGNORECASE)
            if matches:
                patterns.append(BehavioralPattern(
                    pattern_id=f"error_pattern_{pattern_type}",
                    pattern_type="error_disclosure",
                    frequency=len(matches),
                    confidence=0.8,
                    description=f"Unusual error disclosure pattern: {pattern_type}",
                    first_seen=datetime.utcnow(),
                    last_seen=datetime.utcnow()
                ))
        
        return patterns
    
    def _is_unusual_port_combination(self, port_combination: Tuple[int, ...]) -> bool:
        """Check if port combination is unusual"""
        # Define common port combinations
        common_combinations = {
            (80,), (443,), (22,), (21,), (25,), (53,), (110,), (143,),
            (80, 443), (80, 443, 22), (21, 22), (25, 110, 143),
            (80, 443, 21, 22), (80, 443, 22, 25)
        }
        
        # If combination is not in common list and has unusual ports, it's suspicious
        unusual_ports = {1234, 4444, 31337, 8080, 8888, 9999, 12345}
        
        if port_combination not in common_combinations:
            if any(port in unusual_ports for port in port_combination):
                return True
            if len(port_combination) > 10:  # Too many open ports
                return True
        
        return False
    
    def _is_unusual_version(self, service: str, version: str) -> bool:
        """Check if service version is unusual"""
        # Very basic check - in practice, this would reference CVE databases
        suspicious_versions = {
            'apache': ['1.3.x', '2.0.x'],  # Very old versions
            'nginx': ['0.x.x', '1.0.x'],
            'mysql': ['3.x.x', '4.0.x'],
            'php': ['4.x.x', '5.0.x', '5.1.x']
        }
        
        service_lower = service.lower()
        for sus_service, sus_versions in suspicious_versions.items():
            if sus_service in service_lower:
                for sus_version in sus_versions:
                    if version.startswith(sus_version.replace('x', '')):
                        return True
        
        return False
    
    def _is_unusual_response_code(self, code: str, frequency: int) -> bool:
        """Check if HTTP response code pattern is unusual"""
        # Unusual response codes
        unusual_codes = {'402', '418', '420', '450', '499', '509', '520', '521', '522', '523', '524', '525', '526'}
        
        if code in unusual_codes:
            return True
        
        # High frequency of error codes might indicate issues
        if code.startswith('4') or code.startswith('5'):
            if frequency > 50:  # Threshold for unusual frequency
                return True
        
        return False

class ZeroDayDetector:
    """Machine learning-based zero-day vulnerability detection"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.behavioral_analyzer = BehavioralAnalyzer()
        self.models = {}
        self.feature_extractors = {}
        self.scaler = StandardScaler()
        
        # Initialize ML models
        self._initialize_models()
        
    def _initialize_models(self):
        """Initialize machine learning models"""
        try:
            # Anomaly detection model
            self.models['anomaly_detector'] = IsolationForest(
                contamination=0.1,  # 10% anomaly rate
                random_state=42,
                n_estimators=100
            )
            
            # Text feature extractor for vulnerability descriptions
            self.feature_extractors['text'] = TfidfVectorizer(
                max_features=1000,
                stop_words='english',
                ngram_range=(1, 2)
            )
            
            # Clustering model for pattern grouping
            self.models['clustering'] = DBSCAN(
                eps=0.3,
                min_samples=2
            )
            
            self.logger.info("Zero-day detection models initialized")
            
        except Exception as e:
            self.logger.error(f"Error initializing zero-day detection models: {e}")
    
    def detect_potential_zero_days(self, scan_results: Dict[str, Any]) -> Dict[str, Any]:
        """Detect potential zero-day vulnerabilities"""
        try:
            # Extract features from scan results
            features = self._extract_features(scan_results)
            
            # Detect anomalies using ML
            anomalies = self._detect_anomalies(features)
            
            # Analyze behavioral patterns
            behavioral_patterns = self.behavioral_analyzer.analyze_behavioral_patterns(scan_results)
            
            # Correlate findings
            potential_zero_days = self._correlate_findings(anomalies, behavioral_patterns, scan_results)
            
            # Calculate confidence scores
            confidence_scores = self._calculate_confidence_scores(potential_zero_days)
            
            # Generate recommendations
            recommendations = self._generate_recommendations(potential_zero_days)
            
            return {
                'potential_zero_days': potential_zero_days,
                'confidence_scores': confidence_scores,
                'behavioral_patterns': [self._pattern_to_dict(p) for p in behavioral_patterns],
                'recommended_actions': recommendations,
                'analysis_timestamp': datetime.utcnow().isoformat(),
                'total_anomalies_detected': len(anomalies),
                'high_confidence_findings': len([p for p in potential_zero_days if p.confidence_score > 0.7])
            }
            
        except Exception as e:
            self.logger.error(f"Error in zero-day detection: {e}")
            return {
                'potential_zero_days': [],
                'confidence_scores': {},
                'behavioral_patterns': [],
                'recommended_actions': ["Error in zero-day detection system"],
                'analysis_timestamp': datetime.utcnow().isoformat(),
                'total_anomalies_detected': 0,
                'high_confidence_findings': 0,
                'error': str(e)
            }
    
    def _extract_features(self, scan_results: Dict[str, Any]) -> np.ndarray:
        """Extract features from scan results for ML analysis"""
        features = []
        
        # Extract numerical features
        numerical_features = []
        
        # Port-related features
        if 'nmap' in scan_results:
            nmap_output = scan_results['nmap'].get('stdout', '')
            open_ports = re.findall(r'(\d+)/tcp\s+open', nmap_output)
            numerical_features.extend([
                len(open_ports),  # Number of open ports
                len(set(int(p) for p in open_ports if int(p) > 1024)),  # High ports
                len([p for p in open_ports if int(p) < 1024]),  # Well-known ports
            ])
        else:
            numerical_features.extend([0, 0, 0])
        
        # HTTP-related features
        if 'nikto' in scan_results:
            nikto_output = scan_results['nikto'].get('stdout', '')
            error_count = len(re.findall(r'error', nikto_output, re.IGNORECASE))
            vuln_count = len(re.findall(r'vulnerable?', nikto_output, re.IGNORECASE))
            numerical_features.extend([error_count, vuln_count])
        else:
            numerical_features.extend([0, 0])
        
        # Service enumeration features
        if 'enum4linux' in scan_results:
            enum_output = scan_results['enum4linux'].get('stdout', '')
            shares_count = len(re.findall(r'sharename', enum_output, re.IGNORECASE))
            users_count = len(re.findall(r'user:', enum_output, re.IGNORECASE))
            numerical_features.extend([shares_count, users_count])
        else:
            numerical_features.extend([0, 0])
        
        # Ensure we have a consistent feature vector size
        while len(numerical_features) < 20:
            numerical_features.append(0)
        
        features = np.array(numerical_features[:20]).reshape(1, -1)
        
        return features
    
    def _detect_anomalies(self, features: np.ndarray) -> List[Dict[str, Any]]:
        """Detect anomalies using machine learning"""
        anomalies = []
        
        try:
            # Scale features
            features_scaled = self.scaler.fit_transform(features)
            
            # Detect anomalies
            anomaly_score = self.models['anomaly_detector'].fit_predict(features_scaled)
            anomaly_scores = self.models['anomaly_detector'].score_samples(features_scaled)
            
            # If anomaly detected (score = -1)
            if anomaly_score[0] == -1:
                anomalies.append({
                    'type': 'statistical_anomaly',
                    'score': abs(anomaly_scores[0]),
                    'description': 'Statistical anomaly detected in scan results',
                    'features': features[0].tolist(),
                    'confidence': min(abs(anomaly_scores[0]) / 0.5, 1.0)  # Normalize confidence
                })
                
        except Exception as e:
            self.logger.error(f"Error in anomaly detection: {e}")
        
        return anomalies
    
    def _correlate_findings(self, anomalies: List[Dict[str, Any]], 
                          behavioral_patterns: List[BehavioralPattern],
                          scan_results: Dict[str, Any]) -> List[ZeroDayCandidate]:
        """Correlate anomalies and behavioral patterns to identify zero-day candidates"""
        candidates = []
        
        # Create candidates from high-confidence anomalies
        for anomaly in anomalies:
            if anomaly.get('confidence', 0) > 0.6:
                candidate = ZeroDayCandidate(
                    id=f"zeroday_{hashlib.md5(str(anomaly).encode()).hexdigest()[:8]}",
                    confidence_score=anomaly['confidence'],
                    anomaly_type=anomaly['type'],
                    description=f"Potential zero-day vulnerability detected via {anomaly['type']}",
                    indicators=['statistical_anomaly', 'unusual_behavior'],
                    behavioral_patterns=[anomaly['description']],
                    technical_details=anomaly,
                    recommended_actions=[
                        'Manual verification required',
                        'Deep packet inspection',
                        'Source code review if available',
                        'Consult security advisories'
                    ],
                    severity_prediction='medium'
                )
                candidates.append(candidate)
        
        # Create candidates from behavioral pattern clusters
        high_confidence_patterns = [p for p in behavioral_patterns if p.confidence > 0.7]
        
        if len(high_confidence_patterns) >= 2:
            # Multiple high-confidence patterns suggest potential zero-day
            pattern_descriptions = [p.description for p in high_confidence_patterns]
            pattern_types = [p.pattern_type for p in high_confidence_patterns]
            
            candidate = ZeroDayCandidate(
                id=f"zeroday_behavioral_{hashlib.md5(''.join(pattern_descriptions).encode()).hexdigest()[:8]}",
                confidence_score=min(sum(p.confidence for p in high_confidence_patterns) / len(high_confidence_patterns), 1.0),
                anomaly_type='behavioral_correlation',
                description=f"Multiple correlated behavioral anomalies suggest potential zero-day",
                indicators=pattern_types,
                behavioral_patterns=pattern_descriptions,
                technical_details={
                    'pattern_count': len(high_confidence_patterns),
                    'patterns': [self._pattern_to_dict(p) for p in high_confidence_patterns]
                },
                recommended_actions=[
                    'Investigate correlated behavioral patterns',
                    'Check for recent exploit frameworks',
                    'Monitor for similar patterns across network',
                    'Implement additional logging'
                ],
                severity_prediction='high' if len(high_confidence_patterns) > 3 else 'medium'
            )
            candidates.append(candidate)
        
        # Look for specific zero-day indicators
        zero_day_indicators = self._check_zero_day_indicators(scan_results)
        for indicator in zero_day_indicators:
            candidate = ZeroDayCandidate(
                id=f"zeroday_indicator_{hashlib.md5(indicator['type'].encode()).hexdigest()[:8]}",
                confidence_score=indicator['confidence'],
                anomaly_type='indicator_based',
                description=indicator['description'],
                indicators=[indicator['type']],
                behavioral_patterns=[indicator['pattern']],
                technical_details=indicator,
                recommended_actions=indicator['recommendations'],
                severity_prediction=indicator['severity']
            )
            candidates.append(candidate)
        
        return candidates
    
    def _check_zero_day_indicators(self, scan_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Check for specific zero-day indicators"""
        indicators = []
        
        # Check for unusual service banners
        if 'nmap' in scan_results:
            nmap_output = scan_results['nmap'].get('stdout', '')
            
            # Look for custom or modified service banners
            banners = re.findall(r'Service Info:\s+(.+)', nmap_output, re.IGNORECASE)
            for banner in banners:
                if any(keyword in banner.lower() for keyword in ['custom', 'modified', 'patched', 'dev', 'beta']):
                    indicators.append({
                        'type': 'custom_service_banner',
                        'confidence': 0.6,
                        'description': f'Custom or modified service banner detected: {banner}',
                        'pattern': banner,
                        'recommendations': [
                            'Investigate custom service implementation',
                            'Look for undocumented features',
                            'Test for buffer overflows'
                        ],
                        'severity': 'medium'
                    })
        
        # Check for unusual error messages that might indicate new vulnerabilities
        all_errors = ""
        for tool_result in scan_results.values():
            if isinstance(tool_result, dict) and 'stderr' in tool_result:
                all_errors += tool_result['stderr'] + "\n"
        
        # Look for crash indicators
        crash_patterns = [
            r'segmentation\s+fault',
            r'access\s+violation',
            r'buffer\s+overflow',
            r'heap\s+corruption',
            r'stack\s+smashing'
        ]
        
        for pattern in crash_patterns:
            matches = re.findall(pattern, all_errors, re.IGNORECASE)
            if matches:
                indicators.append({
                    'type': 'potential_crash',
                    'confidence': 0.8,
                    'description': f'Potential crash or memory corruption detected',
                    'pattern': f'Pattern: {pattern}',
                    'recommendations': [
                        'Investigate potential memory corruption vulnerability',
                        'Test with fuzzing tools',
                        'Check for buffer overflow conditions'
                    ],
                    'severity': 'high'
                })
        
        return indicators
    
    def _calculate_confidence_scores(self, candidates: List[ZeroDayCandidate]) -> Dict[str, float]:
        """Calculate confidence scores for zero-day candidates"""
        scores = {}
        
        for candidate in candidates:
            # Base confidence from individual analysis
            base_confidence = candidate.confidence_score
            
            # Boost confidence based on multiple indicators
            indicator_boost = min(len(candidate.indicators) * 0.1, 0.3)
            
            # Boost confidence based on behavioral pattern correlation
            pattern_boost = min(len(candidate.behavioral_patterns) * 0.05, 0.2)
            
            # Severity boost
            severity_boost = {
                'critical': 0.2,
                'high': 0.15,
                'medium': 0.1,
                'low': 0.05
            }.get(candidate.severity_prediction, 0)
            
            # Final confidence (capped at 1.0)
            final_confidence = min(
                base_confidence + indicator_boost + pattern_boost + severity_boost,
                1.0
            )
            
            scores[candidate.id] = final_confidence
            candidate.confidence_score = final_confidence
        
        return scores
    
    def _generate_recommendations(self, candidates: List[ZeroDayCandidate]) -> List[str]:
        """Generate recommendations based on zero-day candidates"""
        if not candidates:
            return ["No potential zero-day vulnerabilities detected"]
        
        recommendations = []
        
        # High-confidence candidates require immediate attention
        high_confidence = [c for c in candidates if c.confidence_score > 0.7]
        if high_confidence:
            recommendations.append(
                f"URGENT: {len(high_confidence)} high-confidence zero-day candidates detected - immediate investigation required"
            )
        
        # Specific recommendations based on anomaly types
        anomaly_types = set(c.anomaly_type for c in candidates)
        
        if 'statistical_anomaly' in anomaly_types:
            recommendations.append("Perform deep statistical analysis of network behavior")
        
        if 'behavioral_correlation' in anomaly_types:
            recommendations.append("Investigate correlated behavioral patterns across systems")
        
        if 'indicator_based' in anomaly_types:
            recommendations.append("Validate specific zero-day indicators found")
        
        # General recommendations
        recommendations.extend([
            "Implement enhanced monitoring for detected patterns",
            "Consult latest threat intelligence feeds",
            "Consider engaging security researchers for validation",
            "Update IDS/IPS signatures based on findings"
        ])
        
        return recommendations
    
    def _pattern_to_dict(self, pattern: BehavioralPattern) -> Dict[str, Any]:
        """Convert BehavioralPattern to dictionary"""
        return {
            'pattern_id': pattern.pattern_id,
            'pattern_type': pattern.pattern_type,
            'frequency': pattern.frequency,
            'confidence': pattern.confidence,
            'description': pattern.description,
            'first_seen': pattern.first_seen.isoformat(),
            'last_seen': pattern.last_seen.isoformat()
        }
    
    def save_model_state(self, filepath: str):
        """Save trained models to file"""
        try:
            model_state = {
                'models': self.models,
                'feature_extractors': self.feature_extractors,
                'scaler': self.scaler
            }
            
            with open(filepath, 'wb') as f:
                pickle.dump(model_state, f)
                
            self.logger.info(f"Zero-day detection models saved to {filepath}")
            
        except Exception as e:
            self.logger.error(f"Error saving models: {e}")
    
    def load_model_state(self, filepath: str):
        """Load trained models from file"""
        try:
            with open(filepath, 'rb') as f:
                model_state = pickle.load(f)
            
            self.models = model_state['models']
            self.feature_extractors = model_state['feature_extractors']
            self.scaler = model_state['scaler']
            
            self.logger.info(f"Zero-day detection models loaded from {filepath}")
            
        except Exception as e:
            self.logger.error(f"Error loading models: {e}")
            # Fall back to default initialization
            self._initialize_models()
